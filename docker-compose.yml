# AssistedVoice - Local Development Docker Compose
# For production deployment, see: Containers/vives_net/stacks/tools-stack/assistedvoice/

services:
  assistedvoice:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: assistedvoice-dev
    restart: unless-stopped
    ports:
      - "5001:5001"
    volumes:
      # Persist logs
      - ./logs:/app/logs
      # Persist Whisper models (downloaded on first run)
      - ./models:/app/models
      # Mount config for easy editing
      - ./config.yaml:/app/config.yaml
      # Mount source code for development (Hot Reloading)
      - ./modules:/app/modules
      - ./routers:/app/routers
      - ./services:/app/services
      - ./web_assistant.py:/app/web_assistant.py
      - ./static:/app/static
      - ./templates:/app/templates
    environment:
      - HOST=0.0.0.0
      - PORT=5001
      - FLASK_DEBUG=True
      - SECRET_KEY=dev-secret-change-in-production
      - CORS_ALLOWED_ORIGINS=*
    extra_hosts:
      - "sagan.local:192.168.7.252"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5001/" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

# Note: For LLM connectivity, ensure your config.yaml points to an accessible
# Ollama or LM Studio server (not localhost when running in Docker).
# Example: server.host: "host.docker.internal" or "sagan.local"
