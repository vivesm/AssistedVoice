server:
  type: ollama
  host: host.docker.internal
  port: 11434
  timeout: 30
  retry_attempts: 3
  lm_studio:
    api_key: not-needed
    base_url: /v1
whisper:
  model: small
  language: en
  device: cpu
  compute_type: int8
ollama:
  model: qwen3-vl:8b
  fallback_model: mistral:latest
  temperature: 0.7
  max_tokens: 500
  context_window: 4096
  system_prompt: 'You are a helpful voice assistant. Keep responses concise and natural
    for speech.

    Avoid using markdown, special characters, or formatting that doesn''t work well
    when spoken aloud.

    Be conversational and friendly.

    '
lm_studio:
  model: openai/gpt-oss-20b
  fallback_model: qwen/qwen3-32b
  temperature: 0.7
  max_tokens: 800
  context_window: 8192
  system_prompt: 'You are a helpful voice assistant. Keep responses concise and natural
    for speech.

    Avoid using markdown, special characters, or formatting that doesn''t work well
    when spoken aloud.

    Be conversational and friendly.

    '
ui:
  mode: text
  theme: default
  show_timestamps: true
  show_latency: true
  clear_on_start: true
audio:
  input_device: null
  sample_rate: 16000
  channels: 1
  chunk_duration: 0.5
  silence_threshold: 500
vad:
  enabled: true
  mode: 2
  speech_timeout: 1.5
  min_speech_duration: 0.5
hotkeys:
  push_to_talk: space
  toggle_mode: m
  clear_history: c
  exit: q
logging:
  level: INFO
  file: logs/assistant.log
  max_size: 10MB
  backup_count: 3
performance:
  response_streaming: true
  cache_responses: true
  max_cache_size: 100
  parallel_processing: true
stt:
  type: whisper
  model: small
  language: en
  device: cpu
  compute_type: int8
tts:
  engine: edge-tts
  voice: en-US-JennyNeural
  rate: +0%
  volume: +40%
  edge_voice: en-US-JennyNeural
reading_mode:
  enabled: true
  max_text_length: 200000
  chunk_size: 500
  auto_advance: true
  share_base_url: https://share.vives.io
  fetch_timeout: 10
