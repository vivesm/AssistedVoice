# AssistedVoice - Local Development Docker Compose
# For production deployment, see: Containers/vives_net/stacks/tools-stack/assistedvoice/

services:
  assistedvoice:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: assistedvoice
    restart: unless-stopped
    ports:
      - "5001:5001"
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./data:/app/data
      - ./config.yaml:/app/config.yaml
      - ./signal_data:/app/signal_data
    environment:
      - HOST=0.0.0.0
      - PORT=5001
      - FLASK_DEBUG=False
      - SECRET_KEY=dev-secret-change-in-production
      - CORS_ALLOWED_ORIGINS=*
      - SIGNAL_API_URL=http://signal-api:8080
      - SIGNAL_DATA_PATH=/app/signal_data
      - BACKEND_URL=http://assistedvoice:5001
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - signal-api

  signal-api:
    image: bbernhard/signal-cli-rest-api:latest
    container_name: signal-api
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - ./signal_data/data:/home/.local/share/signal-cli/data
    environment:
      - MODE=json-rpc

# Note: For LLM connectivity, ensure your config.yaml points to an accessible
# Ollama or LM Studio server (not localhost when running in Docker).
# Example: server.host: "host.docker.internal" or "sagan.local"
