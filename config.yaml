audio:
  channels: 1
  chunk_duration: 0.5
  input_device: null
  sample_rate: 16000
  silence_threshold: 500
hotkeys:
  clear_history: c
  exit: q
  push_to_talk: space
  toggle_mode: m
lm_studio:
  context_window: 8192
  fallback_model: qwen/qwen3-32b
  max_tokens: 800
  model: openai/gpt-oss-20b
  system_prompt: 'You are a helpful voice assistant. Keep responses concise and natural
    for speech.

    Avoid using markdown, special characters, or formatting that doesn''t work well
    when spoken aloud.

    Be conversational and friendly.

    '
  temperature: 0.7
logging:
  backup_count: 3
  file: logs/assistant.log
  level: INFO
  max_size: 10MB
ollama:
  context_window: 4096
  fallback_model: mistral:latest
  max_tokens: 500
  model: llama3.2:latest
  system_prompt: 'You are a helpful voice assistant. Keep responses concise and natural
    for speech.

    Avoid using markdown, special characters, or formatting that doesn''t work well
    when spoken aloud.

    Be conversational and friendly.

    '
  temperature: 0.7
  vision_model: ministral-3:8b
performance:
  cache_responses: true
  max_cache_size: 100
  parallel_processing: true
  response_streaming: true
reading_mode:
  auto_advance: true
  chunk_size: 500
  enabled: true
  fetch_timeout: 10
  max_text_length: 200000
  share_base_url: https://share.vives.io
server:
  host: host.docker.internal
  lm_studio:
    api_key: not-needed
    base_url: /v1
  port: 11434
  retry_attempts: 3
  timeout: 30
  type: ollama
stt:
  compute_type: int8
  device: cpu
  language: en
  model: small
  type: whisper
tts:
  edge_voice: en-US-JennyNeural
  engine: edge-tts
  rate: +0%
  voice: en-US-JennyNeural
  volume: +40%
ui:
  clear_on_start: true
  mode: text
  show_latency: true
  show_timestamps: true
  theme: default
vad:
  enabled: true
  min_speech_duration: 0.5
  mode: 2
  speech_timeout: 1.5
whisper:
  compute_type: int8
  device: auto
  language: en
  mode: remote
  model: small
  remote_url: http://192.168.7.224:5001/transcribe
